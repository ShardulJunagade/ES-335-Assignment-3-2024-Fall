{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-10-28T06:05:09.427699Z","iopub.status.busy":"2024-10-28T06:05:09.427382Z","iopub.status.idle":"2024-10-28T06:05:15.980708Z","shell.execute_reply":"2024-10-28T06:05:15.979690Z","shell.execute_reply.started":"2024-10-28T06:05:09.427664Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda\n"]}],"source":["import torch\n","import torch.nn.functional as F\n","from torch import nn\n","import pandas as pd\n","import re\n","from sklearn.model_selection import train_test_split\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","import numpy as np\n","import itertools\n","import os\n","\n","\n","# Set up device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f'Using device: {device}')\n","\n","# Load CSV\n","df = pd.read_csv('data/Cleaned_Indian_Food_Dataset.csv')\n","data = df['TranslatedInstructions']\n","\n","# Check for empty instructions and drop them\n","data = data.dropna().reset_index(drop=True)\n","data = data[data.str.strip().ne(\"\")]  # Keep only non-empty strings\n","\n","# Define special tokens\n","special_tokens = ['start', 'end', 'pad']\n","\n","# Format instructions by adding special tokens\n","formatted_data = [f\"start {instructions} end\" for instructions in data]\n","\n","# Clean and tokenize function with improved punctuation handling\n","def clean_and_tokenize(text):\n","    if text is None or text.strip() == \"\":\n","        return []\n","    \n","    # Keep start and end tokens\n","    text = text.replace('start', ' start ').replace('end', ' end ')\n","    \n","    # Add space before and after punctuation (.,!?)\n","    text = re.sub(r'([.,!?])', r' \\1 ', text)  # Add spaces around punctuation marks\n","\n","    # Remove extra spaces\n","    text = re.sub(r'\\s+', ' ', text).strip()\n","    \n","    # Tokenize by splitting on spaces\n","    segments = text.lower().split()\n","    \n","    return segments  # Return tokenized segments\n","\n","# Apply cleaning and tokenization to formatted data\n","corpus = []\n","for text in formatted_data:\n","    corpus.extend(clean_and_tokenize(text))\n","\n","# Ensure special tokens are included in the corpus\n","corpus.extend(special_tokens)\n","\n","# Create vocabulary mappings\n","vocab = sorted(list(set(corpus)))  # Include special tokens in the vocab\n","word_to_index = {word: idx for idx, word in enumerate(vocab)}\n","index_to_word = {idx: word for idx, word in enumerate(vocab)}\n","\n","# Check that 'pad' token exists in the word_to_index\n","assert 'pad' in word_to_index, \"pad token missing in the vocabulary!\"\n","\n","# Function to create input-output pairs with padding\n","def create_io_pairs(corpus, context_size):\n","    X, y = [], []\n","    for i in range(len(corpus) - context_size):\n","        context = corpus[i:i + context_size]\n","        target = corpus[i + context_size]\n","        \n","        # Pad the context to ensure context_size length\n","        if len(context) < context_size:\n","            context = ['pad'] * (context_size - len(context)) + context\n","        \n","        X.append(context)\n","        y.append(target)\n","    return X, y\n","\n","\n","def create_training_data(context_size, batch_size):\n","    # Create input-output pairs\n","    X, y = create_io_pairs(corpus, context_size)\n","    for i in range(5):\n","        print(X[i], \"->\", y[i])\n","        \n","    # Convert words to indices\n","    X_idx = [[word_to_index[word] for word in sequence] for sequence in X]\n","    Y_idx = [word_to_index[word] for word in y]\n","\n","    # Convert to tensors\n","    X_tensor = torch.tensor(X_idx, dtype=torch.long)\n","    Y_tensor = torch.tensor(Y_idx, dtype=torch.long)\n","\n","    # Split the data into training and testing sets\n","    X_train, X_test, Y_train, Y_test = train_test_split(X_tensor, Y_tensor, test_size=0.2, random_state=42)\n","\n","    # Create a TensorDataset and DataLoader for training\n","    train_dataset = TensorDataset(X_train, Y_train)\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n","\n","    return train_loader, X_test, Y_test\n","\n","\n","# Improved MLP model definition\n","class ImprovedMLP(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, dropout_rate, context_size, activation_function):\n","        super(ImprovedMLP, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.fc1 = nn.Linear(embedding_dim * context_size, hidden_dim)\n","        self.bn1 = nn.BatchNorm1d(hidden_dim)\n","        self.dropout1 = nn.Dropout(dropout_rate)\n","        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n","        self.activation_function = activation_function\n","\n","    def forward(self, x):\n","        x = self.embedding(x).view(x.size(0), -1)\n","        x = self.dropout1(self.activation_function(self.bn1(self.fc1(x))))\n","        x = self.fc2(x)\n","        return F.log_softmax(x, dim=1)\n","\n","# Train and evaluate the model with given parameters\n","def train_model(model, train_loader, criterion, optimizer, num_epochs=10):\n","    model.train()\n","    for epoch in range(num_epochs):\n","        total_loss = 0\n","        for inputs, targets in train_loader:\n","            inputs, targets = inputs.to(device), targets.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, targets)\n","            loss.backward()\n","            optimizer.step()\n","            total_loss += loss.item()\n","        avg_loss = total_loss / len(train_loader)\n","        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}')\n","\n","# Main function to iterate over parameter combinations, train models, and download each\n","def train_multiple_models(context_lengths, embedding_dims, activation_functions, random_seeds, vocab_size, batch_size):\n","    results = []\n","    \n","    # Ensure a directory for saved models\n","    os.makedirs(\"models\", exist_ok=True)\n","\n","    # Generate all combinations of parameters\n","    param_combinations = list(itertools.product(context_lengths, embedding_dims, activation_functions, random_seeds))\n","\n","    for context_size, embedding_dim, activation_fn, random_seed in param_combinations:\n","        torch.manual_seed(random_seed)\n","        np.random.seed(random_seed)\n","\n","        train_loader, X_test, Y_test = create_training_data(context_size, batch_size)\n","        \n","        model = ImprovedMLP(vocab_size, embedding_dim, hidden_dim= 1024, dropout_rate=0.3,\n","                            context_size=context_size, activation_function=activation_fn).to(device)\n","        criterion = nn.NLLLoss()\n","        optimizer = optim.Adam(model.parameters(), lr=0.001)\n","        \n","        print(f\"\\nTraining with context_size={context_size}, embedding_dim={embedding_dim}, \"\n","              f\"activation_fn={activation_fn.__name__}, random_seed={random_seed}\")\n","        \n","        train_model(model, train_loader, criterion, optimizer, num_epochs=500)\n","        \n","        # Save each model\n","        model_filename = f\"models/model_context_{context_size}_emb_{embedding_dim}_act_{activation_fn.__name__}_seed_{random_seed}.pth\"\n","        try:\n","            torch.save(model.state_dict(), model_filename)\n","            print(f\"Model saved to {model_filename}\")\n","        except Exception as e:\n","            print(f\"Error saving model: {e}\")\n","        \n","        results.append(model_filename)\n","        \n","    return results"]},{"cell_type":"markdown","metadata":{},"source":["## hyper params training"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T06:05:15.982803Z","iopub.status.busy":"2024-10-28T06:05:15.982490Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['start', 'to', 'begin', 'making', 'the', 'masala', 'karela', 'recipe', ',', 'de-seed'] -> the\n","['to', 'begin', 'making', 'the', 'masala', 'karela', 'recipe', ',', 'de-seed', 'the'] -> karela\n","['begin', 'making', 'the', 'masala', 'karela', 'recipe', ',', 'de-seed', 'the', 'karela'] -> and\n","['making', 'the', 'masala', 'karela', 'recipe', ',', 'de-seed', 'the', 'karela', 'and'] -> slice\n","['the', 'masala', 'karela', 'recipe', ',', 'de-seed', 'the', 'karela', 'and', 'slice'] -> .\n","\n","Training with context_size=10, embedding_dim=64, activation_fn=leaky_relu, random_seed=42\n","Epoch [1/500], Loss: 4.3076\n","Epoch [2/500], Loss: 3.3545\n","Epoch [3/500], Loss: 3.0228\n","Epoch [4/500], Loss: 2.7979\n","Epoch [5/500], Loss: 2.6362\n","Epoch [6/500], Loss: 2.5225\n","Epoch [7/500], Loss: 2.4332\n","Epoch [8/500], Loss: 2.3607\n","Epoch [9/500], Loss: 2.2984\n","Epoch [10/500], Loss: 2.2447\n","Epoch [11/500], Loss: 2.1985\n","Epoch [12/500], Loss: 2.1551\n","Epoch [13/500], Loss: 2.1192\n","Epoch [14/500], Loss: 2.0833\n","Epoch [15/500], Loss: 2.0527\n","Epoch [16/500], Loss: 2.0246\n","Epoch [17/500], Loss: 1.9971\n","Epoch [18/500], Loss: 1.9729\n","Epoch [19/500], Loss: 1.9490\n","Epoch [20/500], Loss: 1.9278\n","Epoch [21/500], Loss: 1.9064\n","Epoch [22/500], Loss: 1.8883\n","Epoch [23/500], Loss: 1.8696\n","Epoch [24/500], Loss: 1.8517\n","Epoch [25/500], Loss: 1.8359\n","Epoch [26/500], Loss: 1.8204\n","Epoch [27/500], Loss: 1.8039\n","Epoch [28/500], Loss: 1.7901\n","Epoch [29/500], Loss: 1.7765\n","Epoch [30/500], Loss: 1.7641\n","Epoch [31/500], Loss: 1.7505\n","Epoch [32/500], Loss: 1.7386\n","Epoch [33/500], Loss: 1.7262\n","Epoch [34/500], Loss: 1.7144\n","Epoch [35/500], Loss: 1.7039\n","Epoch [36/500], Loss: 1.6934\n","Epoch [37/500], Loss: 1.6821\n","Epoch [38/500], Loss: 1.6725\n","Epoch [39/500], Loss: 1.6615\n"]}],"source":["# Example parameter lists\n","context_lengths = [10]\n","embedding_dims = [64]\n","activation_functions = [F.leaky_relu]\n","random_seeds = [42]\n","batch_size = 4096\n","\n","# Call the function to train models on all combinations and download them\n","results = train_multiple_models(context_lengths, embedding_dims, activation_functions, random_seeds, len(vocab), batch_size)\n","print(\"Models saved:\", results)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # Save the test data for the specific context size\n","#     test_data_path = f\"test_data_context_{context_size}.pt\"\n","#     torch.save((X_test, Y_test), test_data_path)\n","#     print(f\"Test data for context size {context_size} saved to {test_data_path}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Predicting"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn.functional as F\n","import json\n","\n","# Load vocabulary mappings\n","with open(\"word_to_index.json\", \"r\") as f:\n","    word_to_index = json.load(f)\n","\n","with open(\"index_to_word.json\", \"r\") as f:\n","    index_to_word = json.load(f)\n","\n","vocab_size = len(word_to_index)  # Ensure this matches the vocab size used for training\n","context_size = 10  # Adjust if you used a different context size\n","embedding_dim = 32  # Match with the specific model's embedding dimension\n","activation_function_name = \"leaky_relu\"  # String-based variable for flexibility\n","seed = 42\n","\n","# Map the string to the actual activation function\n","activation_function_map = {\n","    \"tanh\": torch.tanh,\n","    \"relu\": F.relu,\n","    \"leaky_relu\": F.leaky_relu\n","}\n","activation_function = activation_function_map.get(activation_function_name, F.relu)\n","\n","# Initialize device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Define the model\n","model = ImprovedMLP(vocab_size, embedding_dim, hidden_dim= 1024, dropout_rate=0.3,\n","                    context_size=context_size, activation_function=activation_function).to(device)\n","\n","# Adjust the model path to match your saved model filename\n","model_path = f\"models/model_context_{context_size}_emb_{embedding_dim}_act_{activation_function_name}_seed_{seed}.pth\"\n","\n","# Load the trained weights\n","try:\n","    model.load_state_dict(torch.load(model_path, map_location=device))\n","    model.eval()  # Set model to evaluation mode\n","    print(\"Model loaded successfully!\")\n","except FileNotFoundError:\n","    print(f\"Model file not found at {model_path}. Ensure the file exists.\")\n","except Exception as e:\n","    print(f\"Error loading model: {e}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Define the helper to convert words to indices if not already defined\n","def words_to_indices(words, word_to_index):\n","    return [word_to_index[word] if word in word_to_index else word_to_index['pad'] for word in words]\n","\n","# Define your start sequence in words\n","start_sequence_words = \"Mix milk and cream\"  # Example start sequence\n","start_sequence_words = clean_and_tokenize(start_sequence_words)\n","start_sequence_indices = words_to_indices(start_sequence_words, word_to_index)\n","\n","# Pad the start sequence to the context size\n","if len(start_sequence_indices) < context_size:\n","    start_sequence_indices = [word_to_index['pad']] * (context_size - len(start_sequence_indices)) + start_sequence_indices\n","\n","print(\"Start Sequence (in indices):\", start_sequence_indices)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Generate text\n","def generate_text(model, start_sequence, num_words, temperature=1.0):\n","    model.eval()\n","    generated = list(start_sequence)\n","    for _ in range(num_words):\n","        input_seq = torch.tensor(generated[-context_size:], dtype=torch.long).unsqueeze(0).to(device)\n","        with torch.no_grad():\n","            output = model(input_seq)\n","        logits = output.squeeze(0)\n","        logits = logits / temperature\n","        probs = F.softmax(logits, dim=-1)\n","        next_word_idx = torch.multinomial(probs, num_samples=1).item()\n","        generated.append(next_word_idx)\n","        if index_to_word[next_word_idx] == 'end':\n","            break\n","    generated_words = [index_to_word[idx] for idx in generated if index_to_word[idx] != 'pad']\n","    return generated_words\n","\n","# Generate and print text\n","generated_text = generate_text(model, start_sequence_indices, num_words= 100, temperature= 1)\n","print(\"Generated Recipe:\", ' '.join(generated_text))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Define the start sequence in words\n","start_sequence_words = \"Take some paneer and add chocolate\"  # Example start sequence\n","start_sequence_words = clean_and_tokenize(start_sequence_words)\n","start_sequence_indices = words_to_indices(start_sequence_words, word_to_index)\n","\n","# Pad the start sequence to the context size\n","if len(start_sequence_indices) < context_size:\n","    start_sequence_indices = [word_to_index['pad']] * (context_size - len(start_sequence_indices)) + start_sequence_indices\n","\n","print(\"Start Sequence (in indices):\", start_sequence_indices)\n","\n","# Generate and print text\n","generated_text = generate_text(model, start_sequence_indices, num_words= 100, temperature= 1)\n","print(\"Generated Recipe:\", ' '.join(generated_text))"]},{"cell_type":"markdown","metadata":{},"source":["## Saving"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import json\n","\n","with open(\"word_to_index.json\", \"w\") as f:\n","    json.dump(word_to_index, f)\n","\n","with open(\"index_to_word.json\", \"w\") as f:\n","    json.dump(index_to_word, f)\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":1218948,"sourceId":2035629,"sourceType":"datasetVersion"}],"dockerImageVersionId":30786,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
