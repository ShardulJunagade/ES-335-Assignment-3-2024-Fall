{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-10-28T06:05:09.427699Z","iopub.status.busy":"2024-10-28T06:05:09.427382Z","iopub.status.idle":"2024-10-28T06:05:15.980708Z","shell.execute_reply":"2024-10-28T06:05:15.979690Z","shell.execute_reply.started":"2024-10-28T06:05:09.427664Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cpu\n"]}],"source":["import torch\n","import torch.nn.functional as F\n","from torch import nn\n","import pandas as pd\n","import re\n","from sklearn.model_selection import train_test_split\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","import numpy as np\n","import itertools\n","import os\n","\n","\n","# Set up device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f'Using device: {device}')\n","\n","# Load CSV\n","df = pd.read_csv('data/Cleaned_Indian_Food_Dataset.csv')\n","data = df['TranslatedInstructions']\n","\n","# Check for empty instructions and drop them\n","data = data.dropna().reset_index(drop=True)\n","data = data[data.str.strip().ne(\"\")]  # Keep only non-empty strings\n","\n","# Define special tokens\n","special_tokens = ['start', 'end', 'pad']\n","\n","# Format instructions by adding special tokens\n","formatted_data = [f\"start {instructions} end\" for instructions in data]\n","\n","# Clean and tokenize function with improved punctuation handling\n","def clean_and_tokenize(text):\n","    if text is None or text.strip() == \"\":\n","        return []\n","    \n","    # Keep start and end tokens\n","    text = text.replace('start', ' start ').replace('end', ' end ')\n","    \n","    # Add space before and after punctuation (.,!?)\n","    text = re.sub(r'([.,!?])', r' \\1 ', text)  # Add spaces around punctuation marks\n","\n","    # Remove extra spaces\n","    text = re.sub(r'\\s+', ' ', text).strip()\n","    \n","    # Tokenize by splitting on spaces\n","    segments = text.lower().split()\n","    \n","    return segments  # Return tokenized segments\n","\n","# Apply cleaning and tokenization to formatted data\n","corpus = []\n","for text in formatted_data:\n","    corpus.extend(clean_and_tokenize(text))\n","\n","# Ensure special tokens are included in the corpus\n","corpus.extend(special_tokens)\n","\n","# Create vocabulary mappings\n","vocab = sorted(list(set(corpus)))  # Include special tokens in the vocab\n","word_to_index = {word: idx for idx, word in enumerate(vocab)}\n","index_to_word = {idx: word for idx, word in enumerate(vocab)}\n","\n","# Check that 'pad' token exists in the word_to_index\n","assert 'pad' in word_to_index, \"pad token missing in the vocabulary!\"\n","\n","# Function to create input-output pairs with padding\n","def create_io_pairs(corpus, context_size):\n","    X, y = [], []\n","    for i in range(len(corpus) - context_size):\n","        context = corpus[i:i + context_size]\n","        target = corpus[i + context_size]\n","        \n","        # Pad the context to ensure context_size length\n","        if len(context) < context_size:\n","            context = ['pad'] * (context_size - len(context)) + context\n","        \n","        X.append(context)\n","        y.append(target)\n","    return X, y\n","\n","\n","def create_training_data(context_size, batch_size):\n","    # Create input-output pairs\n","    X, y = create_io_pairs(corpus, context_size)\n","    for i in range(5):\n","        print(X[i], \"->\", y[i])\n","        \n","    # Convert words to indices\n","    X_idx = [[word_to_index[word] for word in sequence] for sequence in X]\n","    Y_idx = [word_to_index[word] for word in y]\n","\n","    # Convert to tensors\n","    X_tensor = torch.tensor(X_idx, dtype=torch.long)\n","    Y_tensor = torch.tensor(Y_idx, dtype=torch.long)\n","\n","    # Split the data into training and testing sets\n","    X_train, X_test, Y_train, Y_test = train_test_split(X_tensor, Y_tensor, test_size=0.2, random_state=42)\n","\n","    # Create a TensorDataset and DataLoader for training\n","    train_dataset = TensorDataset(X_train, Y_train)\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n","\n","    test_data_path = f\"assets/test_data_context_{context_size}.pt\"\n","    torch.save((X_test, Y_test), test_data_path)\n","    print(f\"Test data for context size {context_size} saved to {test_data_path}\")\n","\n","\n","    return train_loader, X_test, Y_test\n","\n","\n","# Improved MLP model definition\n","class ImprovedMLP(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, dropout_rate, context_size, activation_function):\n","        super(ImprovedMLP, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.fc1 = nn.Linear(embedding_dim * context_size, hidden_dim)\n","        self.bn1 = nn.BatchNorm1d(hidden_dim)\n","        self.dropout1 = nn.Dropout(dropout_rate)\n","        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n","        self.activation_function = activation_function\n","\n","    def forward(self, x):\n","        x = self.embedding(x).view(x.size(0), -1)\n","        x = self.dropout1(self.activation_function(self.bn1(self.fc1(x))))\n","        x = self.fc2(x)\n","        return F.log_softmax(x, dim=1)\n","\n","# Train and evaluate the model with given parameters\n","def train_model(model, train_loader, criterion, optimizer, num_epochs=10):\n","    model.train()\n","    for epoch in range(num_epochs):\n","        total_loss = 0\n","        for inputs, targets in train_loader:\n","            inputs, targets = inputs.to(device), targets.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, targets)\n","            loss.backward()\n","            optimizer.step()\n","            total_loss += loss.item()\n","        avg_loss = total_loss / len(train_loader)\n","        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}')\n","\n","# Main function to iterate over parameter combinations, train models, and download each\n","def train_multiple_models(context_lengths, embedding_dims, activation_functions, random_seeds, vocab_size, batch_size):\n","    results = []\n","    \n","    # Ensure a directory for saved models\n","    os.makedirs(\"models\", exist_ok=True)\n","\n","    # Generate all combinations of parameters\n","    param_combinations = list(itertools.product(context_lengths, embedding_dims, activation_functions, random_seeds))\n","\n","    for context_size, embedding_dim, activation_fn, random_seed in param_combinations:\n","        torch.manual_seed(random_seed)\n","        np.random.seed(random_seed)\n","\n","        train_loader, X_test, Y_test = create_training_data(context_size, batch_size)\n","        \n","        model = ImprovedMLP(vocab_size, embedding_dim, hidden_dim= 1024, dropout_rate=0.3,\n","                            context_size=context_size, activation_function=activation_fn).to(device)\n","        criterion = nn.NLLLoss()\n","        optimizer = optim.Adam(model.parameters(), lr=0.001)\n","        \n","        print(f\"\\nTraining with context_size={context_size}, embedding_dim={embedding_dim}, \"\n","              f\"activation_fn={activation_fn.__name__}, random_seed={random_seed}\")\n","        \n","        train_model(model, train_loader, criterion, optimizer, num_epochs=500)\n","        \n","        # Save each model\n","        model_filename = f\"models/model_context_{context_size}_emb_{embedding_dim}_act_{activation_fn.__name__}_seed_{random_seed}.pth\"\n","        try:\n","            torch.save(model.state_dict(), model_filename)\n","            print(f\"Model saved to {model_filename}\")\n","        except Exception as e:\n","            print(f\"Error saving model: {e}\")\n","        \n","        results.append(model_filename)\n","        \n","    return results"]},{"cell_type":"markdown","metadata":{},"source":["## hyper params training"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T06:05:15.982803Z","iopub.status.busy":"2024-10-28T06:05:15.982490Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['start', 'to', 'begin', 'making', 'the', 'masala', 'karela', 'recipe', ',', 'de-seed'] -> the\n","['to', 'begin', 'making', 'the', 'masala', 'karela', 'recipe', ',', 'de-seed', 'the'] -> karela\n","['begin', 'making', 'the', 'masala', 'karela', 'recipe', ',', 'de-seed', 'the', 'karela'] -> and\n","['making', 'the', 'masala', 'karela', 'recipe', ',', 'de-seed', 'the', 'karela', 'and'] -> slice\n","['the', 'masala', 'karela', 'recipe', ',', 'de-seed', 'the', 'karela', 'and', 'slice'] -> .\n","Test data for context size 10 saved to assets/test_data_context_10.pt\n","\n","Training with context_size=10, embedding_dim=64, activation_fn=leaky_relu, random_seed=0\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[4], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4096\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Call the function to train models on all combinations and download them\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_multiple_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation_functions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_seeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModels saved:\u001b[39m\u001b[38;5;124m\"\u001b[39m, results)\n","Cell \u001b[1;32mIn[2], line 169\u001b[0m, in \u001b[0;36mtrain_multiple_models\u001b[1;34m(context_lengths, embedding_dims, activation_functions, random_seeds, vocab_size, batch_size)\u001b[0m\n\u001b[0;32m    164\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining with context_size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, embedding_dim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00membedding_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    167\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactivation_fn=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactivation_fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, random_seed=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrandom_seed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 169\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# Save each model\u001b[39;00m\n\u001b[0;32m    172\u001b[0m model_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/model_context_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_emb_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00membedding_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_act_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactivation_fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_seed_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrandom_seed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n","Cell \u001b[1;32mIn[2], line 134\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m    133\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 134\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\Soham\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[1;32mc:\\Users\\Soham\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1332\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\Soham\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1295\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1291\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1292\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1295\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1296\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1297\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n","File \u001b[1;32mc:\\Users\\Soham\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[0;32m   1121\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[0;32m   1122\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[0;32m   1131\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1133\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[0;32m   1135\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1136\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\Soham\\anaconda3\\envs\\pytorch_env\\Lib\\multiprocessing\\queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[0;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[1;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n","File \u001b[1;32mc:\\Users\\Soham\\anaconda3\\envs\\pytorch_env\\Lib\\multiprocessing\\connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[1;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\Soham\\anaconda3\\envs\\pytorch_env\\Lib\\multiprocessing\\connection.py:346\u001b[0m, in \u001b[0;36mPipeConnection._poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_got_empty_message \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m    344\u001b[0m             _winapi\u001b[38;5;241m.\u001b[39mPeekNamedPipe(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n","File \u001b[1;32mc:\\Users\\Soham\\anaconda3\\envs\\pytorch_env\\Lib\\multiprocessing\\connection.py:1084\u001b[0m, in \u001b[0;36mwait\u001b[1;34m(object_list, timeout)\u001b[0m\n\u001b[0;32m   1081\u001b[0m                 ready_objects\u001b[38;5;241m.\u001b[39madd(o)\n\u001b[0;32m   1082\u001b[0m                 timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1084\u001b[0m     ready_handles \u001b[38;5;241m=\u001b[39m \u001b[43m_exhaustive_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaithandle_to_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;66;03m# request that overlapped reads stop\u001b[39;00m\n\u001b[0;32m   1087\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ov \u001b[38;5;129;01min\u001b[39;00m ov_list:\n","File \u001b[1;32mc:\\Users\\Soham\\anaconda3\\envs\\pytorch_env\\Lib\\multiprocessing\\connection.py:1016\u001b[0m, in \u001b[0;36m_exhaustive_wait\u001b[1;34m(handles, timeout)\u001b[0m\n\u001b[0;32m   1014\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m L:\n\u001b[1;32m-> 1016\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWaitForMultipleObjects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;241m==\u001b[39m WAIT_TIMEOUT:\n\u001b[0;32m   1018\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Example parameter lists\n","context_lengths = [10]\n","embedding_dims = [64]\n","activation_functions = [F.leaky_relu]\n","random_seeds = [0]\n","batch_size = 4096\n","\n","# Call the function to train models on all combinations and download them\n","results = train_multiple_models(context_lengths, embedding_dims, activation_functions, random_seeds, len(vocab), batch_size)\n","print(\"Models saved:\", results)"]},{"cell_type":"markdown","metadata":{},"source":["## Predicting"]},{"cell_type":"code","execution_count":6,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model loaded successfully!\n"]}],"source":["import torch\n","import torch.nn.functional as F\n","import json\n","\n","# Load vocabulary mappings\n","with open(\"assets/word_to_index.json\", \"r\") as f:\n","    word_to_index = json.load(f)\n","\n","with open(\"assets/index_to_word.json\", \"r\") as f:\n","    index_to_word = json.load(f)\n","\n","vocab_size = len(word_to_index)  # Ensure this matches the vocab size used for training\n","context_size = 10  # Adjust if you used a different context size\n","embedding_dim = 32  # Match with the specific model's embedding dimension\n","activation_function_name = \"leaky_relu\"  # String-based variable for flexibility\n","seed = 42\n","\n","# Map the string to the actual activation function\n","activation_function_map = {\n","    \"tanh\": torch.tanh,\n","    \"relu\": F.relu,\n","    \"leaky_relu\": F.leaky_relu\n","}\n","activation_function = activation_function_map.get(activation_function_name, F.relu)\n","\n","# Initialize device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Define the model\n","model = ImprovedMLP(vocab_size, embedding_dim, hidden_dim= 1024, dropout_rate=0.3,\n","                    context_size=context_size, activation_function=activation_function).to(device)\n","\n","# Adjust the model path to match your saved model filename\n","model_path = f\"models/model_context_{context_size}_emb_{embedding_dim}_act_{activation_function_name}_seed_{seed}.pth\"\n","\n","# Load the trained weights\n","try:\n","    model.load_state_dict(torch.load(model_path, map_location=device))\n","    model.eval()  # Set model to evaluation mode\n","    print(\"Model loaded successfully!\")\n","except FileNotFoundError:\n","    print(f\"Model file not found at {model_path}. Ensure the file exists.\")\n","except Exception as e:\n","    print(f\"Error loading model: {e}\")\n"]},{"cell_type":"code","execution_count":11,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Start Sequence (in indices): [9286, 9286, 9286, 9286, 9286, 9286, 8390, 8286, 1696, 3913]\n"]}],"source":["# Define the helper to convert words to indices if not already defined\n","def words_to_indices(words, word_to_index):\n","    return [word_to_index[word] if word in word_to_index else word_to_index['pad'] for word in words]\n","\n","# Define your start sequence in words\n","start_sequence_words = \"Mix milk and cream\"  # Example start sequence\n","start_sequence_words = clean_and_tokenize(start_sequence_words)\n","start_sequence_indices = words_to_indices(start_sequence_words, word_to_index)\n","\n","# Pad the start sequence to the context size\n","if len(start_sequence_indices) < context_size:\n","    start_sequence_indices = [word_to_index['pad']] * (context_size - len(start_sequence_indices)) + start_sequence_indices\n","\n","print(\"Start Sequence (in indices):\", start_sequence_indices)\n"]},{"cell_type":"code","execution_count":16,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Generated Recipe: mix milk and cream on hot water . cook until the milk is reduced to half a lid . add 1/2 cup of water to cook covered for 5 minutes on low heat . turn off the heat after two whistles cooked , once the pressure releases . switch off the flame , cool a little and make a smooth and smooth batter . if you have a stove . heat a teaspoon of oil in a heavy bottomed pan . when they are hot , add the ghee and cumin seeds . add in the capsicum leaves , onion , bell pepper ,\n"]}],"source":["import torch\n","import torch.nn.functional as F\n","from torch import nn\n","import json\n","import re\n","\n","# Load vocabulary mappings (ensure one-time loading)\n","with open(\"assets/word_to_index.json\", \"r\") as f:\n","    word_to_index = json.load(f)\n","\n","\n","with open(\"assets/index_to_word.json\", \"r\") as f:\n","    index_to_word = json.load(f)\n","    index_to_word = {int(k): v for k, v in index_to_word.items()}\n","\n","\n","vocab_size = len(word_to_index)  # Make sure it matches training vocab size\n","\n","# Set up device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","\n","# Text generation function\n","def generate_text(model, start_sequence, num_words, temperature=1.0):\n","    model.eval()\n","    generated = list(start_sequence)\n","    for _ in range(num_words):\n","        input_seq = torch.tensor(generated[-context_size:], dtype=torch.long).unsqueeze(0).to(device)\n","        with torch.no_grad():\n","            output = model(input_seq)\n","        logits = output.squeeze(0) / temperature\n","        next_word_idx = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1).item()\n","        generated.append(next_word_idx)\n","        if index_to_word[next_word_idx] == 'end':\n","            break\n","    return ' '.join(index_to_word[idx] for idx in generated if index_to_word[idx] != 'pad')\n","\n","# Pad the sequence to match context size\n","if len(start_sequence_indices) < context_size:\n","    start_sequence_indices = [word_to_index['pad']] * (context_size - len(start_sequence_indices)) + start_sequence_indices\n","#Generate and print text\n","generated_text = generate_text(model, start_sequence_indices, num_words= 100, temperature= 1)\n","print(\"Generated Recipe:\", ''.join(generated_text))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Define the start sequence in words\n","start_sequence_words = \"Take some paneer and add chocolate\"  # Example start sequence\n","start_sequence_words = clean_and_tokenize(start_sequence_words)\n","start_sequence_indices = words_to_indices(start_sequence_words, word_to_index)\n","\n","# Pad the start sequence to the context size\n","if len(start_sequence_indices) < context_size:\n","    start_sequence_indices = [word_to_index['pad']] * (context_size - len(start_sequence_indices)) + start_sequence_indices\n","\n","print(\"Start Sequence (in indices):\", start_sequence_indices)\n","\n","# Generate and print text\n","generated_text = generate_text(model, start_sequence_indices, num_words= 100, temperature= 1)\n","print(\"Generated Recipe:\", ' '.join(generated_text))"]},{"cell_type":"markdown","metadata":{},"source":["## Saving"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import json\n","\n","with open(\"assets/word_to_index.json\", \"w\") as f:\n","    json.dump(word_to_index, f)\n","\n","with open(\"assets/index_to_word.json\", \"w\") as f:\n","    json.dump(index_to_word, f)\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":1218948,"sourceId":2035629,"sourceType":"datasetVersion"}],"dockerImageVersionId":30786,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"pytorch_env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":4}
